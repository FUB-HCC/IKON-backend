{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "source": [
    "# Topic extraction from the GEPRiS dataset and creation of an user-centric visualisation\n",
    "Author: Tim Korjakow        \n",
    "Summer term 2018      \n",
    "Freie Universit√§t Berlin     \n",
    "Fachgebiet Human-Centered Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "source": [
    "![Process graph](nlpflowchart.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.4.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.4.0.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# general imports\n",
    "import numpy as np\n",
    "#import sklearn\n",
    "import os\n",
    "\n",
    "# data wrangling\n",
    "import json\n",
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "import psycopg2\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from os import path\n",
    "\n",
    "\n",
    "\n",
    "# document embedding\n",
    "from gensim.utils import SaveLoad\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.sklearn_api import TfIdfTransformer, D2VTransformer\n",
    "from gensim.parsing.preprocessing import preprocess_string, STOPWORDS, strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, strip_short\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy\n",
    "\n",
    "# topic extraction\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "#clustering\n",
    "from numpy import triu_indices\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, FeatureAgglomeration\n",
    "from sklearn.neighbors import radius_neighbors_graph\n",
    "\n",
    "# projection into 2d\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# linearization\n",
    "from lapjv import lapjv\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.interpolate import griddata\n",
    "from numpy.linalg import norm\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "# quality metrics of the clustering\n",
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "# pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from tempfile import mkdtemp\n",
    "\n",
    "# interactivity\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider, Dropdown, FloatSlider, fixed\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Javascript, HTML\n",
    "import pickle\n",
    "\n",
    "#2d plot\n",
    "from bokeh.io import output_notebook, show, export_png\n",
    "from bokeh.models import ColumnDataSource, OpenURL, TapTool, LinearAxis, Grid\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models.glyphs import VBar\n",
    "from bokeh.palettes import d3, brewer, mpl, inferno\n",
    "from bokeh.layouts import row, column\n",
    "output_notebook()\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# 3d plot\n",
    "import plotly.express as px\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "init_notebook_mode(connected=True) # for offline mode use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "source": [
    "## Loading and Cleaning\n",
    "The first step in every NLP project which works with texts is always the preparation of the input data. In this example the Project dump from GEPRIS is loaded and the project descriptions are extracted. After that the texts get cleaned by removing all non-alphabetic chars and all stopwords from the texts. English texts are getting filtered in oder to make the analysis simpler and more comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "outputs": [],
   "source": [
    "with open(os.environ['PG_PASSWORD']) as password_file:\n",
    "    password = password_file.read().strip()\n",
    "    conn = psycopg2.connect(dbname=\"ikon\", user=\"ikonuser\", password=password, port=5432, host='Postgres')\n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self, query, clean=True, stream=False, workers=cpu_count()):\n",
    "        self.query = query\n",
    "        self.data = self.loadFromDB(self.query).fetchall()\n",
    "            \n",
    "    def __iter__(self):\n",
    "        self.pos = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.pos >= len(self.data):\n",
    "            raise StopIteration\n",
    "        self.pos += 1\n",
    "        return self.__getitem__(self.pos-1)\n",
    "    \n",
    "    def  __getitem__(self, pos):\n",
    "        return self.data[self.pos]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data)\n",
    "    \n",
    "    def loadFromDB(self, query):\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        return cursor\n",
    "        \n",
    "class DataPreprocessor(DataLoader):\n",
    "    def __init__(self, query, clean=True, stream=False, workers=cpu_count()):\n",
    "        self.query = query\n",
    "        self.clean = clean\n",
    "        self.nlp = spacy.load('de', disable=[\"ner\", \"tagger\"])\n",
    "        self.nlp.Defaults.stop_words |= self.loadEnglishStopwords()\n",
    "        data = self.chunkify(self.loadFromDB(self.query).fetchall(), workers)\n",
    "        with Pool(workers) as pool:\n",
    "            self.data = [item for sublist in pool.map(self.preprocessText, data) for item in sublist]\n",
    "    \n",
    "    def getIDs(self):\n",
    "        return [id for (text, id, title) in self.data]\n",
    "    \n",
    "    def getTitles(self):\n",
    "        return [title for (text, id, title) in self.data]\n",
    "        \n",
    "    def  __getitem__(self, pos):\n",
    "        text, *args = self.data[pos]\n",
    "        return text\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.data)\n",
    "\n",
    "    def loadEnglishStopwords(self):\n",
    "        with open('../data/stopwords_eng.json', 'r') as datafile:\n",
    "            return set(json.load(datafile))\n",
    "        \n",
    "    def preprocessText(self, results):\n",
    "        texts, *args = zip(*results)\n",
    "        data = []\n",
    "        for doc, *args in zip(self.nlp.pipe(texts, batch_size=100, n_threads=-1), *args):\n",
    "            if doc.lang_ == 'de' and len(doc) > 0:\n",
    "                filter_doc = tuple([token.lemma_ for token in doc if self.filterType(token)])\n",
    "                if len(filter_doc) > 0:\n",
    "                    data.append((filter_doc, *args))\n",
    "        return data\n",
    "    \n",
    "    def chunkify(self, lst, n):\n",
    "        return [lst[i::n] for i in range(n)]\n",
    "        \n",
    "    def filterType(self, token):\n",
    "        return token.is_alpha and not (token.is_stop or token.like_num or token.is_punct) and len(token.lemma_) > 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.78 s, sys: 577 ms, total: 6.35 s\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "traindata = %time DataPreprocessor('''SELECT FIRST(project_abstract), FIRST(id), FIRST(title) \\\n",
    "                                FROM projects \\\n",
    "                                WHERE project_abstract NOT LIKE '%Keine Zusammenfassung%' \\\n",
    "                                GROUP BY project_abstract \\\n",
    "                                LIMIT 100;''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfndata = DataPreprocessor('''SELECT summary, id, titelprojekt \\\n",
    "                        FROM mfnprojects \\\n",
    "                        WHERE summary NOT LIKE '%Zusammenfassung%';''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Document Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "source": [
    "### TF-IDF\n",
    "*Summary*:\n",
    "This technique vectorizes a corpus, e.g. a collection of documents, by counting all appearences of words in the corpus and computing the tf-idf measure for each document, word pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<99x100 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 9900 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = Embedding(method='doc2vec')\n",
    "emb.fit_transform(traindata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "source": [
    "## Latent Semantic Analysis\n",
    "*Summary*:\n",
    "The LSA transforms an corpus from its word space given by the tf-idf matrice into its semantic space. In this semantic space the dimensions denote topics in the corpus and every document vector is a linear combination of all the implicitly extracted topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x80\\x03c__main__\\nTopicExtraction\\nq\\x00)\\x81q\\x01}q\\x02(X\\x08\\x00\\x00\\x00featuresq\\x03K\\nX\\x06\\x00\\x00\\x00methodq\\x04X\\x03\\x00\\x00\\x00lsaq\\x05X\\x08\\x00\\x00\\x00selectorq\\x06csklearn.decomposition.truncated_svd\\nTruncatedSVD\\nq\\x07)\\x81q\\x08}q\\t(X\\t\\x00\\x00\\x00algorithmq\\nX\\n\\x00\\x00\\x00randomizedq\\x0bX\\x0c\\x00\\x00\\x00n_componentsq\\x0cK\\nX\\x06\\x00\\x00\\x00n_iterq\\rK\\x05X\\x0c\\x00\\x00\\x00random_stateq\\x0eK\\x00X\\x03\\x00\\x00\\x00tolq\\x0fG\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00X\\x10\\x00\\x00\\x00_sklearn_versionq\\x10X\\x06\\x00\\x00\\x000.21.3q\\x11ubub.'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = TopicExtraction(10)\n",
    "pickle.dumps(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "source": [
    "### K-Means\n",
    "Summary: Given a clustering the LDA can be used to find a projection into a lower dimensional space which maximizes inter-class variance and minimizes intra-class variance. This leads to neater cluster, but is grounded in the hypotheses that the clusters have some real semantic meaning. Otherwise it may enforce preexisting biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterNumberHeuristic(tfs):\n",
    "    return (tfs.shape[0]*tfs.shape[1])//tfs.count_nonzero()\n",
    "\n",
    "def clusterkm(tfs_reduced, num_topics=10):\n",
    "    km = KMeans(n_clusters=num_topics).fit(tfs_reduced)\n",
    "    return km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "source": [
    "# Embedding into 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "source": [
    "## Linear Discriminant Analysis\n",
    "*Summary*:\n",
    "Given a clustering the LDA can be used to find a projection into a lower dimensional space which maximizes inter-class variance and minimizes intra-class variance. This leads to neater cluster, but is grounded in the hypotheses that the clusters have some real semantic meaning. Otherwise it may enforce preexisting biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "source": [
    "## tSNE\n",
    "*Summary*:\n",
    "\n",
    "\n",
    "*In-depth explanation*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linearize results into a grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapToSpaceSampling(points):\n",
    "    # just take the first n¬≤ < #points Points\n",
    "    points = points[: int(np.sqrt(len(points)))**2]\n",
    "    grid = np.dstack(np.meshgrid(np.linspace(np.min(points[:, 0]), np.max(points[:, 0]), int(np.sqrt(len(points)))),\n",
    "                       np.linspace(np.min(points[:, 1]), np.max(points[:, 1]), int(np.sqrt(len(points)))))).reshape(-1, 2)\n",
    "    cost = cdist(points, grid, \"sqeuclidean\").astype(np.float64)\n",
    "    cost *= 100000 / cost.max()\n",
    "    row_ind_lapjv, col_ind_lapjv, _ = lapjv(cost, verbose=True, force_doubles=True)\n",
    "    return grid[row_ind_lapjv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeClusterTopography(points, values, width, height, interpolation='linear'):\n",
    "    # lay grid over the points so that all points are covered\n",
    "    grid_x, grid_y = np.mgrid[np.min(points[:,0]):np.max(points[:,0]):width*1j, np.min(points[:,1]):np.max(points[:,1]):height*1j]\n",
    "    return griddata(np.array(points), np.array(values[:len(points)]), (grid_x, grid_y), method=interpolation, fill_value=np.min(values[:len(points)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(embedding, dimreduction, clustering, planereduction, num_topics, num_clusters, perplexity, learning_rate, error, interpolation, viz, width, height):\n",
    "    \n",
    "    estimators = [('Embedding', Embedding(method=embedding)),\n",
    "                  ('EmbeddingData', Debug()),\n",
    "                  ('TopicExtraction', TopicExtraction(num_topics, method=dimreduction)),\n",
    "                  ('TopicExtractionData', Debug()),\n",
    "                  ('Clustering', Clustering(num_clusters, method=clustering)), \n",
    "                  ('PlaneReduction', PlaneReduction(2, method=planereduction, perplexity=perplexity, learning_rate=learning_rate))]\n",
    "    pipe = Pipeline(estimators, memory='/tmp/')\n",
    "    \n",
    "    pipe.fit(traindata)\n",
    "    tfs_plane, labels = pipe.fit_transform(mfndata)\n",
    "    tfs_reduced = pipe.named_steps.TopicExtractionData.data\n",
    "    tfs = pipe.named_steps.EmbeddingData.data\n",
    "    print(tfs.shape)\n",
    "    \n",
    "    # compute linearization\n",
    "    tfs_mapped = mapToSpaceSampling(tfs_plane)\n",
    "    \n",
    "    # compute top words\n",
    "    cluster_words = [pipe.named_steps.Embedding.top_words(csr_matrix(vstack([tfs.getrow(i) for i in np.flatnonzero(labels==cluster)]).mean(axis=0)), topn=5) for cluster in range(num_clusters)]\n",
    "    top_words = [pipe.named_steps.Embedding.top_words(tfs.getrow(i), topn=5) for i in range(tfs.shape[0])]\n",
    "    # compute coherence score\n",
    "    #cm = CoherenceModel(topics=cluster_words, window_size=10, texts=[list(doc.words) for doc in traindata], dictionary=dct, processes=cpu_count())\n",
    "    \n",
    "    #compute cluster topography\n",
    "    similarity_to_cluster_centers = silhouette_samples(tfs_plane, labels=labels)\n",
    "\n",
    "    interpolated_topography = np.array([1]*(width*height))# computeClusterTopography(tfs_plane if viz == 'scatter' else tfs_mapped, silhouette_samples(tfs_reduced, clusters.labels_), width, height, interpolation)\n",
    "\n",
    "    return tfs_reduced, labels, tfs_plane, tfs_mapped, cluster_words, top_words, similarity_to_cluster_centers, interpolated_topography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_scatter(data, width=600, height=600, viz='scatter'):\n",
    "    display(Javascript(\"\"\"\n",
    "        (function(element){\n",
    "            require(['scatter'], function(scatter) {\n",
    "                scatter(element.get(0), %s, %d, %d, %s);\n",
    "            });\n",
    "        })(element);\n",
    "    \"\"\" % (json.dumps(data), width, height, json.dumps(viz))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "outputs": [],
   "source": [
    "def save(payload):\n",
    "    name = \"c\" + str(payload['params']['num_clusters']) +\"-t\" + str(payload['params']['num_topics']) + \"_\" + str(payload['params']['embedding'])\n",
    "    if payload['params']['embedding'] == 'tSNE':\n",
    "        name += \"_p\" + str(payload['params']['perplexity']) + \"-lr\" + str(payload['params']['learning_rate'])\n",
    "    with open('./dumps/' + name + '.json', 'w') as dumpfile:\n",
    "        json.dump(payload, dumpfile, sort_keys=True, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "outputs": [],
   "source": [
    "def visualize(embedding='tfidf',dimreduction='LSA', clustering='KMEANS', planereduction='LDA', num_topics=20, granularity=5, perplexity=5, learning_rate=200, error='cluster_error', interpolation='linear', viz='scatter', fake=''):\n",
    "    # viz dimensions\n",
    "    num_clusters=granularity\n",
    "    width = 600\n",
    "    height = 600\n",
    "    payload = {}\n",
    "    \n",
    "    \n",
    "    if not fake:\n",
    "        tfs_reduced, clusters, tfs_embedded, tfs_mapped, cluster_words, top_words, similarity_to_cluster_centers, interpolated_topography = compute(embedding, dimreduction, clustering, planereduction, num_topics, num_clusters, perplexity, learning_rate, error, interpolation, viz, width, height)\n",
    "\n",
    "        [print(i, words) for i,words in enumerate(cluster_words)]\n",
    "        colours = d3['Category20'][num_clusters]\n",
    "        \n",
    "        # configure bokeh plot                   \n",
    "        source = ColumnDataSource(data=dict(\n",
    "            x=tfs_embedded[:, 0],\n",
    "            y=tfs_embedded[:, 1],\n",
    "            x_mapped=tfs_mapped[:, 0],\n",
    "            y_mapped=tfs_mapped[:, 1],\n",
    "            ids=mfndata.getIDs(),\n",
    "            titles=mfndata.getTitles(),\n",
    "            colours=np.array(colours)[clusters],\n",
    "            labels=clusters\n",
    "        ))\n",
    "\n",
    "        TOOLTIPS = [\n",
    "            (\"index\", \"$index\"),\n",
    "            (\"id\", \"@ids\"),\n",
    "            (\"title\", \"@titles\"),\n",
    "        ]\n",
    "        # scatterplot\n",
    "        scatter = figure(plot_width=800, plot_height=800, title=None, toolbar_location=\"below\", tooltips=TOOLTIPS, tools='tap,pan,wheel_zoom,save')\n",
    "        scatter.scatter('x', 'y', size=10,color='colours', legend='labels', source=source)\n",
    "        url = 'http://gepris.dfg.de/gepris/projekt/@ids'\n",
    "        taptool = scatter.select(type=TapTool)\n",
    "        taptool.callback = OpenURL(url=url)\n",
    "\n",
    "        # mapped scatterplot\n",
    "        mapped_scatter = figure(plot_width=800, plot_height=800, title=None, toolbar_location=\"below\", tooltips=TOOLTIPS, tools='tap,pan,wheel_zoom')\n",
    "        mapped_scatter.scatter('x_mapped', 'y_mapped', size=50,color='colours', legend='labels', source=source)\n",
    "        url = 'http://gepris.dfg.de/gepris/projekt/@ids'\n",
    "        taptool = mapped_scatter.select(type=TapTool)\n",
    "        taptool.callback = OpenURL(url=url)\n",
    "\n",
    "        payload = {\n",
    "            'params': {\n",
    "                'dimreduction': dimreduction,\n",
    "                'clustering': clustering,\n",
    "                'embedding': embedding,\n",
    "                'num_topics': num_topics,\n",
    "                'num_clusters': num_clusters,\n",
    "                'perplexity': perplexity,\n",
    "                'learning_rate': learning_rate\n",
    "            },\n",
    "            'project_data': [{'id':pid,'reducedpoint': reducedpoint, 'embpoint':embpoint, 'mappoint':mappoint, 'cluster':cluster, 'error':error, 'title': title, 'words': words} for pid, reducedpoint, embpoint, mappoint, cluster, error, title, words in zip(\n",
    "                mfndata.getIDs(),\n",
    "                tfs_reduced.tolist(),\n",
    "                tfs_embedded.tolist(),\n",
    "                tfs_mapped.tolist(),\n",
    "                clusters.tolist(),\n",
    "                similarity_to_cluster_centers.tolist(),\n",
    "                mfndata.getTitles(),\n",
    "                top_words\n",
    "\n",
    "            )],\n",
    "            'cluster_data': {\n",
    "                'cluster_words': cluster_words,\n",
    "                'cluster_colour': colours\n",
    "            },\n",
    "            'cluster_topography': np.flip(interpolated_topography.T, axis=0).flatten().tolist()\n",
    "        }\n",
    "        save(payload)\n",
    "    else:\n",
    "        with open(fake, 'r') as input_data:\n",
    "            payload = payload=json.load(input_data)\n",
    "    display(HTML(filename=\"scatter.css.html\"))\n",
    "    display(Javascript(\"require.config({paths: {d3: 'https://d3js.org/d3.v5.min'}});\"))\n",
    "    display(Javascript(filename=\"scatter.js\"))\n",
    "    draw_scatter(payload, width, height, viz)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "local_metadata": {
     "scrolled": false
    },
    "remote_metadata": {
     "scrolled": false,
     "tags": [
      "remove_cell"
     ]
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d373bd84a534cada2b7d5cbd7d2273d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='embedding', index=1, options=('doc2vec', 'tfidf'), value='tfidf'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def s(x,y):\n",
    "    return IntSlider(min=x,max=y, value=(y-x)//2, continuous_update=False)\n",
    "\n",
    "w = interactive(visualize, embedding=['doc2vec', 'tfidf'], dimreduction=['lsa'], clustering=['kmeans'], planereduction=['lda','tsne'], num_topics=s(4,48), num_clusters=s(4,10), perplexity=s(5,50), learning_rate=s(100,1000),error=['silhouette', 'cluster_error'], interpolation=['linear', 'cubic', 'nearest'], viz=['scatter', 'linearized'], fake='')\n",
    "output = w.children[-1]\n",
    "#output.layout.height = '2000px'\n",
    "display(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "ikon",
   "language": "python",
   "name": "ikon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}