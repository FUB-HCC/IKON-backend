{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "source": [
    "# Topic extraction from the GEPRiS dataset and creation of an user-centric visualisation\n",
    "Author: Tim Korjakow        \n",
    "Summer term 2018      \n",
    "Freie Universit√§t Berlin     \n",
    "Fachgebiet Human-Centered Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "source": [
    "![Process graph](nlpflowchart.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.2.0.min.js\"];\n",
       "  var css_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.2.0.min.css\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.2.0.min.css\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.2.0.min.css\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.2.0.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.2.0.min.js\"];\n  var css_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.2.0.min.css\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.2.0.min.css\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.2.0.min.css\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# general imports\n",
    "import numpy as np\n",
    "#import sklearn\n",
    "import os\n",
    "\n",
    "# data wrangling\n",
    "import json\n",
    "import spacy\n",
    "spacy.prefer_gpu()\n",
    "from spacy_langdetect import LanguageDetector\n",
    "import psycopg2\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# document embedding\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import corpus2csc\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.parsing.preprocessing import preprocess_string, STOPWORDS, strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, strip_short\n",
    "import scipy\n",
    "\n",
    "# topic extraction\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF as NonnegativeMatrixFactorization\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import keras\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "#clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# projection into 2d\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# linearization\n",
    "from lapjv import lapjv\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.interpolate import griddata\n",
    "from numpy.linalg import norm\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# quality metrics of the clustering\n",
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "# interactivity\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, IntSlider, Dropdown\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Javascript, HTML\n",
    "import pickle\n",
    "\n",
    "#2d plot\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.models import ColumnDataSource, OpenURL, TapTool\n",
    "from bokeh.plotting import figure, ColumnDataSource\n",
    "from bokeh.palettes import d3, brewer\n",
    "from bokeh.layouts import row, column\n",
    "output_notebook()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 3d plot\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "init_notebook_mode(connected=True) # for offline mode use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "source": [
    "## Loading and Cleaning\n",
    "The first step in every NLP project which works with texts is always the preparation of the input data. In this example the Project dump from GEPRIS is loaded and the project descriptions are extracted. After that the texts get cleaned by removing all non-alphabetic chars and all stopwords from the texts. English texts are getting filtered in oder to make the analysis simpler and more comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "outputs": [],
   "source": [
    "with open(os.environ['PG_PASSWORD']) as password_file:\n",
    "    password = password_file.read().strip()\n",
    "    conn = psycopg2.connect(dbname=\"ikon\", user=\"ikonuser\", password=password, port=5432, host='Postgres')\n",
    "class DataLoader(object):\n",
    "    def __init__(self, query, clean=True, stream=False, workers=cpu_count()):\n",
    "        self.query = query\n",
    "        self.clean = clean\n",
    "        self.nlp = spacy.load('de', disable=[\"ner\", \"tagger\"])\n",
    "        #self.nlp.add_pipe(self.nlp.create_pipe('sentencizer'))\n",
    "        self.nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
    "        self.nlp.Defaults.stop_words |= self.loadGermanStopwords()\n",
    "        self.nlp.Defaults.stop_words |= self.loadEnglishStopwords()\n",
    "        data = self.chunkify(self.loadFromDB(self.query).fetchall(), workers)\n",
    "        with Pool(workers) as pool:\n",
    "            self.data = [item for sublist in pool.map(self.preprocessText, data) for item in sublist]\n",
    "        \n",
    "        self.filepath = get_tmpfile(str(hash(tuple(self.data))))\n",
    "        with open(self.filepath, \"w\") as file:\n",
    "            for text, *args in self.data:\n",
    "                file.write(\"%s\\n\" % \" \".join(text))\n",
    "            \n",
    "    def __iter__(self):\n",
    "        self.pos = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.pos >= len(self.data):\n",
    "            raise StopIteration\n",
    "        text, *args = self.data[self.pos]\n",
    "        self.pos += 1\n",
    "        return TaggedDocument(text, [self.pos])\n",
    "    \n",
    "    def  __getitem__(self, pos):\n",
    "        text, *args = self.data[pos]\n",
    "        return TaggedDocument(text, [pos])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def getIDs(self):\n",
    "        return [id for (text, id, title) in self.data]\n",
    "    \n",
    "    def getTitles(self):\n",
    "        return [title for (text, id, title) in self.data]\n",
    "        \n",
    "    def loadFromDB(self, query):\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        return cursor\n",
    "        \n",
    "    def loadGermanStopwords(self):\n",
    "        with open('../data/stopwords_de.json', 'r') as datafile:\n",
    "            return set(json.load(datafile))\n",
    "\n",
    "    def loadEnglishStopwords(self):\n",
    "        with open('../data/stopwords_eng.json', 'r') as datafile:\n",
    "            return set(json.load(datafile))\n",
    "        \n",
    "    def preprocessText(self, results):\n",
    "        texts, *args = zip(*results)\n",
    "        data = []\n",
    "        for doc, *args in zip(self.nlp.pipe(texts, batch_size=100, n_threads=-1), *args):\n",
    "            if(doc._.language['language'] == 'de'):\n",
    "                data.append((tuple([token.lemma_ for token in doc if self.filterType(token)]), *args))\n",
    "        return data\n",
    "    \n",
    "    def chunkify(self, lst, n):\n",
    "        return [lst[i::n] for i in range(n)]\n",
    "        \n",
    "    def filterType(self, token):\n",
    "        return token.is_alpha and not (token.is_stop or token.like_num or token.is_punct) and len(token.lemma_) > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.91 s, sys: 1.67 s, total: 7.58 s\n",
      "Wall time: 23min 21s\n"
     ]
    }
   ],
   "source": [
    "traindata = %time DataLoader('''SELECT FIRST(project_abstract), FIRST(id), FIRST(title) \\\n",
    "                                FROM projects WHERE project_abstract NOT LIKE '%Keine Zusammenfassung%' \\\n",
    "                                GROUP BY project_abstract \\\n",
    "                                ;''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfndata = DataLoader('''SELECT summary, id, titelprojekt \\\n",
    "                        FROM mfnprojects \\\n",
    "                        WHERE summary NOT LIKE '%Zusammenfassung%';''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Document Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "source": [
    "### TF-IDF\n",
    "*Summary*:\n",
    "This technique vectorizes a corpus, e.g. a collection of documents, by counting all appearences of words in the corpus and computing the tf-idf measure for each document, word pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfModelExtended(TfidfModel):\n",
    "    def top_words(self, vector, dct=None ,topn=5):\n",
    "        if isinstance(vector, scipy.sparse.csr_matrix):\n",
    "            vector = vector.todense()\n",
    "        return [dct.get(entry) for entry in np.argpartition(np.asarray(vector).ravel(), -topn)[-topn:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.5 s, sys: 104 ms, total: 15.6 s\n",
      "Wall time: 15.6 s\n",
      "CPU times: user 5.19 s, sys: 5.74 ms, total: 5.2 s\n",
      "Wall time: 5.2 s\n"
     ]
    }
   ],
   "source": [
    "dct = %time Dictionary(doc.words for doc in traindata)  # fit dictionary\n",
    "traincorpus = [dct.doc2bow(doc.words) for doc in traindata]  # convert corpus to BoW format\n",
    "tfidf_model = %time TfidfModelExtended(traincorpus)  # fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfncorpus = [dct.doc2bow(doc.words) for doc in mfndata]  # convert corpus to BoW format\n",
    "docs_vectorized_tfidf = corpus2csc(tfidf_model[mfncorpus]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec\n",
    "*Summary*:\n",
    "This technique vectorizes a corpus, e.g. a collection of documents, by counting all appearences of words in the corpus and computing the tf-idf measure for each document, word pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecExtended(Doc2Vec):\n",
    "    def top_words(self, vector, dct=None, topn=5):\n",
    "        return [word for word, prob in self.wv.similar_by_vector(vector, topn=topn)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec setup and vocabulary building:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tf/.venv/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning:\n",
      "\n",
      "This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 54s, sys: 2.06 s, total: 12min 56s\n",
      "Wall time: 3min 28s\n",
      "Doc2Vec training:\n",
      "CPU times: user 12min 41s, sys: 2.02 s, total: 12min 43s\n",
      "Wall time: 3min 19s\n"
     ]
    }
   ],
   "source": [
    "print('Doc2Vec setup and vocabulary building:')\n",
    "doc2vec_model = %time Doc2VecExtended(corpus_file=traindata.filepath, total_words=dct.num_pos, vector_size=100, window=20, min_count=10, workers=4, epochs=20)\n",
    "print('Doc2Vec training:')\n",
    "%time doc2vec_model.train(corpus_file=traindata.filepath, total_words=dct.num_pos, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_vectorized_doc2vec = np.array([doc2vec_model.infer_vector(doc.words) for doc in mfndata])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76, 100)\n"
     ]
    }
   ],
   "source": [
    "print(docs_vectorized_doc2vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    },
    "toc-hr-collapsed": false
   },
   "source": [
    "# Topic extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "source": [
    "## Latent Semantic Analysis\n",
    "*Summary*:\n",
    "The LSA transforms an corpus from its word space given by the tf-idf matrice into its semantic space. In this semantic space the dimensions denote topics in the corpus and every document vector is a linear combination of all the implicitly extracted topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSA(tfs,num_topics=40):\n",
    "    print('LSA:')\n",
    "    lsa = %time TruncatedSVD(n_components=num_topics, random_state=0).fit(tfs)\n",
    "    return lsa.transform(tfs), lsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    },
    "toc-hr-collapsed": false
   },
   "source": [
    "## Autoencoder\n",
    "Summary: **Coming soon**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0809 11:23:45.057801 140118268290880 deprecation_wrapper.py:119] From /tf/.venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0809 11:23:45.116694 140118268290880 deprecation_wrapper.py:119] From /tf/.venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0809 11:23:45.123407 140118268290880 deprecation_wrapper.py:119] From /tf/.venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0809 11:23:45.164699 140118268290880 deprecation_wrapper.py:119] From /tf/.venv/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 40  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "input_dim = doc2vec_model.docvecs.vectors_docs.shape[1]\n",
    "print(input_dim)\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(input_dim,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0809 11:23:45.396359 140118268290880 deprecation_wrapper.py:119] From /tf/.venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0809 11:23:45.404826 140118268290880 deprecation_wrapper.py:119] From /tf/.venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 82292 samples, validate on 76 samples\n",
      "Epoch 1/150\n",
      "82292/82292 [==============================] - 1s 11us/step - loss: 0.1176 - val_loss: 0.0090\n",
      "Epoch 2/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 0.0035 - val_loss: 0.0017\n",
      "Epoch 3/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 9.7840e-04 - val_loss: 9.3627e-04\n",
      "Epoch 4/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 5.2874e-04 - val_loss: 6.8639e-04\n",
      "Epoch 5/150\n",
      "82292/82292 [==============================] - 1s 8us/step - loss: 3.5463e-04 - val_loss: 5.6805e-04\n",
      "Epoch 6/150\n",
      "82292/82292 [==============================] - 1s 8us/step - loss: 2.6369e-04 - val_loss: 5.0016e-04\n",
      "Epoch 7/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 2.0837e-04 - val_loss: 4.5655e-04\n",
      "Epoch 8/150\n",
      "82292/82292 [==============================] - 1s 8us/step - loss: 1.7140e-04 - val_loss: 4.2637e-04\n",
      "Epoch 9/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.4506e-04 - val_loss: 4.0434e-04\n",
      "Epoch 10/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.2542e-04 - val_loss: 3.8762e-04\n",
      "Epoch 11/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.1024e-04 - val_loss: 3.7452e-04\n",
      "Epoch 12/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 9.8178e-05 - val_loss: 3.6401e-04\n",
      "Epoch 13/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 8.8382e-05 - val_loss: 3.5540e-04\n",
      "Epoch 14/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 8.0278e-05 - val_loss: 3.4824e-04\n",
      "Epoch 15/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 7.3469e-05 - val_loss: 3.4218e-04\n",
      "Epoch 16/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 6.7672e-05 - val_loss: 3.3701e-04\n",
      "Epoch 17/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 6.2683e-05 - val_loss: 3.3254e-04\n",
      "Epoch 18/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 5.8345e-05 - val_loss: 3.2864e-04\n",
      "Epoch 19/150\n",
      "82292/82292 [==============================] - 1s 8us/step - loss: 5.4541e-05 - val_loss: 3.2521e-04\n",
      "Epoch 20/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 5.1180e-05 - val_loss: 3.2218e-04\n",
      "Epoch 21/150\n",
      "82292/82292 [==============================] - 1s 8us/step - loss: 4.8190e-05 - val_loss: 3.1948e-04\n",
      "Epoch 22/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 4.5515e-05 - val_loss: 3.1706e-04\n",
      "Epoch 23/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 4.3107e-05 - val_loss: 3.1487e-04\n",
      "Epoch 24/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 4.0929e-05 - val_loss: 3.1290e-04\n",
      "Epoch 25/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 3.8951e-05 - val_loss: 3.1110e-04\n",
      "Epoch 26/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 3.7146e-05 - val_loss: 3.0947e-04\n",
      "Epoch 27/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 3.5493e-05 - val_loss: 3.0796e-04\n",
      "Epoch 28/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 3.3975e-05 - val_loss: 3.0658e-04\n",
      "Epoch 29/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 3.2575e-05 - val_loss: 3.0531e-04\n",
      "Epoch 30/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 3.1280e-05 - val_loss: 3.0414e-04\n",
      "Epoch 31/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 3.0080e-05 - val_loss: 3.0305e-04\n",
      "Epoch 32/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 2.8964e-05 - val_loss: 3.0203e-04\n",
      "Epoch 33/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 2.7924e-05 - val_loss: 3.0109e-04\n",
      "Epoch 34/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 2.6953e-05 - val_loss: 3.0020e-04\n",
      "Epoch 35/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 2.6044e-05 - val_loss: 2.9938e-04\n",
      "Epoch 36/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 2.5192e-05 - val_loss: 2.9860e-04\n",
      "Epoch 37/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 2.4391e-05 - val_loss: 2.9788e-04\n",
      "Epoch 38/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 2.3637e-05 - val_loss: 2.9719e-04\n",
      "Epoch 39/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 2.2926e-05 - val_loss: 2.9655e-04\n",
      "Epoch 40/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 2.2255e-05 - val_loss: 2.9594e-04\n",
      "Epoch 41/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 2.1620e-05 - val_loss: 2.9536e-04\n",
      "Epoch 42/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 2.1018e-05 - val_loss: 2.9482e-04\n",
      "Epoch 43/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 2.0448e-05 - val_loss: 2.9430e-04\n",
      "Epoch 44/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.9906e-05 - val_loss: 2.9381e-04\n",
      "Epoch 45/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.9392e-05 - val_loss: 2.9335e-04\n",
      "Epoch 46/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.8901e-05 - val_loss: 2.9291e-04\n",
      "Epoch 47/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.8434e-05 - val_loss: 2.9248e-04\n",
      "Epoch 48/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.7989e-05 - val_loss: 2.9208e-04\n",
      "Epoch 49/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.7563e-05 - val_loss: 2.9170e-04\n",
      "Epoch 50/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.7156e-05 - val_loss: 2.9133e-04\n",
      "Epoch 51/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.6767e-05 - val_loss: 2.9098e-04\n",
      "Epoch 52/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.6394e-05 - val_loss: 2.9064e-04\n",
      "Epoch 53/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.6037e-05 - val_loss: 2.9032e-04\n",
      "Epoch 54/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.5694e-05 - val_loss: 2.9001e-04\n",
      "Epoch 55/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.5365e-05 - val_loss: 2.8972e-04\n",
      "Epoch 56/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.5049e-05 - val_loss: 2.8943e-04\n",
      "Epoch 57/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.4745e-05 - val_loss: 2.8916e-04\n",
      "Epoch 58/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.4452e-05 - val_loss: 2.8890e-04\n",
      "Epoch 59/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.4170e-05 - val_loss: 2.8865e-04\n",
      "Epoch 60/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.3899e-05 - val_loss: 2.8840e-04\n",
      "Epoch 61/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.3637e-05 - val_loss: 2.8817e-04\n",
      "Epoch 62/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.3385e-05 - val_loss: 2.8794e-04\n",
      "Epoch 63/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.3141e-05 - val_loss: 2.8773e-04\n",
      "Epoch 64/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.2906e-05 - val_loss: 2.8752e-04\n",
      "Epoch 65/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.2679e-05 - val_loss: 2.8731e-04\n",
      "Epoch 66/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.2459e-05 - val_loss: 2.8712e-04\n",
      "Epoch 67/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.2246e-05 - val_loss: 2.8693e-04\n",
      "Epoch 68/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.2040e-05 - val_loss: 2.8674e-04\n",
      "Epoch 69/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.1841e-05 - val_loss: 2.8657e-04\n",
      "Epoch 70/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.1647e-05 - val_loss: 2.8639e-04\n",
      "Epoch 71/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.1460e-05 - val_loss: 2.8623e-04\n",
      "Epoch 72/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.1278e-05 - val_loss: 2.8607e-04\n",
      "Epoch 73/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.1102e-05 - val_loss: 2.8591e-04\n",
      "Epoch 74/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.0931e-05 - val_loss: 2.8576e-04\n",
      "Epoch 75/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.0765e-05 - val_loss: 2.8561e-04\n",
      "Epoch 76/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.0604e-05 - val_loss: 2.8547e-04\n",
      "Epoch 77/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.0447e-05 - val_loss: 2.8533e-04\n",
      "Epoch 78/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.0294e-05 - val_loss: 2.8519e-04\n",
      "Epoch 79/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.0146e-05 - val_loss: 2.8506e-04\n",
      "Epoch 80/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 1.0002e-05 - val_loss: 2.8494e-04\n",
      "Epoch 81/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 9.8615e-06 - val_loss: 2.8481e-04\n",
      "Epoch 82/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 9.7250e-06 - val_loss: 2.8469e-04\n",
      "Epoch 83/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 9.5919e-06 - val_loss: 2.8457e-04\n",
      "Epoch 84/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 9.4622e-06 - val_loss: 2.8446e-04\n",
      "Epoch 85/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 9.3360e-06 - val_loss: 2.8435e-04\n",
      "Epoch 86/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 9.2127e-06 - val_loss: 2.8424e-04\n",
      "Epoch 87/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 9.0926e-06 - val_loss: 2.8413e-04\n",
      "Epoch 88/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 8.9756e-06 - val_loss: 2.8403e-04\n",
      "Epoch 89/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 8.8613e-06 - val_loss: 2.8393e-04\n",
      "Epoch 90/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 8.7497e-06 - val_loss: 2.8383e-04\n",
      "Epoch 91/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 8.6409e-06 - val_loss: 2.8374e-04\n",
      "Epoch 92/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 8.5347e-06 - val_loss: 2.8365e-04\n",
      "Epoch 93/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 8.4307e-06 - val_loss: 2.8355e-04\n",
      "Epoch 94/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 8.3293e-06 - val_loss: 2.8347e-04\n",
      "Epoch 95/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 8.2301e-06 - val_loss: 2.8338e-04\n",
      "Epoch 96/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 8.1331e-06 - val_loss: 2.8329e-04\n",
      "Epoch 97/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 8.0382e-06 - val_loss: 2.8321e-04\n",
      "Epoch 98/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 7.9456e-06 - val_loss: 2.8313e-04\n",
      "Epoch 99/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 7.8550e-06 - val_loss: 2.8305e-04\n",
      "Epoch 100/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 7.7662e-06 - val_loss: 2.8298e-04\n",
      "Epoch 101/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 7.6794e-06 - val_loss: 2.8290e-04\n",
      "Epoch 102/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 7.5945e-06 - val_loss: 2.8283e-04\n",
      "Epoch 103/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 7.5114e-06 - val_loss: 2.8275e-04\n",
      "Epoch 104/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 7.4300e-06 - val_loss: 2.8268e-04\n",
      "Epoch 105/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 7.3501e-06 - val_loss: 2.8261e-04\n",
      "Epoch 106/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 7.2720e-06 - val_loss: 2.8255e-04\n",
      "Epoch 107/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 7.1954e-06 - val_loss: 2.8248e-04\n",
      "Epoch 108/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 7.1203e-06 - val_loss: 2.8241e-04\n",
      "Epoch 109/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 7.0466e-06 - val_loss: 2.8235e-04\n",
      "Epoch 110/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 6.9744e-06 - val_loss: 2.8229e-04\n",
      "Epoch 111/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 6.9036e-06 - val_loss: 2.8223e-04\n",
      "Epoch 112/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 6.8341e-06 - val_loss: 2.8217e-04\n",
      "Epoch 113/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 6.7661e-06 - val_loss: 2.8211e-04\n",
      "Epoch 114/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 6.6993e-06 - val_loss: 2.8205e-04\n",
      "Epoch 115/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 6.6337e-06 - val_loss: 2.8199e-04\n",
      "Epoch 116/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 6.5695e-06 - val_loss: 2.8194e-04\n",
      "Epoch 117/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 6.5064e-06 - val_loss: 2.8189e-04\n",
      "Epoch 118/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 6.4444e-06 - val_loss: 2.8183e-04\n",
      "Epoch 119/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 6.3836e-06 - val_loss: 2.8178e-04\n",
      "Epoch 120/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 6.3238e-06 - val_loss: 2.8173e-04\n",
      "Epoch 121/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 6.2652e-06 - val_loss: 2.8168e-04\n",
      "Epoch 122/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 6.2075e-06 - val_loss: 2.8163e-04\n",
      "Epoch 123/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 6.1509e-06 - val_loss: 2.8158e-04\n",
      "Epoch 124/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 6.0952e-06 - val_loss: 2.8153e-04\n",
      "Epoch 125/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 6.0405e-06 - val_loss: 2.8149e-04\n",
      "Epoch 126/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 5.9866e-06 - val_loss: 2.8144e-04\n",
      "Epoch 127/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 5.9337e-06 - val_loss: 2.8139e-04\n",
      "Epoch 128/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 5.8817e-06 - val_loss: 2.8135e-04\n",
      "Epoch 129/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 5.8305e-06 - val_loss: 2.8131e-04\n",
      "Epoch 130/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 5.7802e-06 - val_loss: 2.8126e-04\n",
      "Epoch 131/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 5.7307e-06 - val_loss: 2.8122e-04\n",
      "Epoch 132/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 5.6821e-06 - val_loss: 2.8118e-04\n",
      "Epoch 133/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 5.6341e-06 - val_loss: 2.8114e-04\n",
      "Epoch 134/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 5.5869e-06 - val_loss: 2.8110e-04\n",
      "Epoch 135/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 5.5404e-06 - val_loss: 2.8106e-04\n",
      "Epoch 136/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 5.4949e-06 - val_loss: 2.8102e-04\n",
      "Epoch 137/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 5.4501e-06 - val_loss: 2.8098e-04\n",
      "Epoch 138/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 5.4060e-06 - val_loss: 2.8095e-04\n",
      "Epoch 139/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 5.3625e-06 - val_loss: 2.8091e-04\n",
      "Epoch 140/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 5.3196e-06 - val_loss: 2.8087e-04\n",
      "Epoch 141/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 5.2772e-06 - val_loss: 2.8084e-04\n",
      "Epoch 142/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 5.2354e-06 - val_loss: 2.8080e-04\n",
      "Epoch 143/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82292/82292 [==============================] - 1s 7us/step - loss: 5.1943e-06 - val_loss: 2.8077e-04\n",
      "Epoch 144/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 5.1539e-06 - val_loss: 2.8073e-04\n",
      "Epoch 145/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 5.1142e-06 - val_loss: 2.8070e-04\n",
      "Epoch 146/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 5.0751e-06 - val_loss: 2.8067e-04\n",
      "Epoch 147/150\n",
      "82292/82292 [==============================] - 1s 6us/step - loss: 5.0366e-06 - val_loss: 2.8063e-04\n",
      "Epoch 148/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 4.9986e-06 - val_loss: 2.8060e-04\n",
      "Epoch 149/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 4.9612e-06 - val_loss: 2.8057e-04\n",
      "Epoch 150/150\n",
      "82292/82292 [==============================] - 1s 7us/step - loss: 4.9241e-06 - val_loss: 2.8054e-04\n"
     ]
    }
   ],
   "source": [
    "input_train = normalize(doc2vec_model.docvecs.vectors_docs, axis=0, norm='l1')\n",
    "input_test = normalize(docs_vectorized_doc2vec, axis=0, norm='l1')\n",
    "history = autoencoder.fit(input_train, input_train,\n",
    "                epochs=150,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(input_test, input_test),\n",
    "                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate topic extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "source": [
    "## Get top words for each dimension\n",
    "In order to get the words which are most important for each dimension (which correspond to topics), the standard basis in the topic space is converted back into the word space. These are exactly the eigenvectors of data. Now the top n biggest entries and their corresponding words form the top words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words_dim(model, feature_names, n_top_words):\n",
    "    dim_topics = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        dim_topics[topic_idx] = [feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "    return dim_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "source": [
    "### K-Means\n",
    "Summary: Given a clustering the LDA can be used to find a projection into a lower dimensional space which maximizes inter-class variance and minimizes intra-class variance. This leads to neater cluster, but is grounded in the hypotheses that the clusters have some real semantic meaning. Otherwise it may enforce preexisting biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterNumberHeuristic(tfs):\n",
    "    return (tfs.shape[0]*tfs.shape[1])//tfs.count_nonzero()\n",
    "\n",
    "def cluster(tfs_reduced, num_topics=10):\n",
    "    print('K-Means:')\n",
    "    km = %time KMeans(n_clusters=num_topics).fit(tfs_reduced)\n",
    "    return km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "source": [
    "### Get top words for each cluster\n",
    "The process is similar to the one for getting the top words for each dimension. But in this case the cluster centers from the clustering step are transformed back into the word space and analysed. This is based on the assumption that the cluster center represents the set of all documents in the corrsponding cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words_point(model, point, feature_names, n_top_words):\n",
    "    point = np.array(point).reshape(1,-1)\n",
    "    point.reshape(1, -1)\n",
    "    word_space_point = model.inverse_transform(point)\n",
    "    return [feature_names[j] for j in word_space_point.argsort()[0][:-n_top_words - 1:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "source": [
    "# Embedding into 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "source": [
    "## Linear Discriminant Analysis\n",
    "*Summary*:\n",
    "Given a clustering the LDA can be used to find a projection into a lower dimensional space which maximizes inter-class variance and minimizes intra-class variance. This leads to neater cluster, but is grounded in the hypotheses that the clusters have some real semantic meaning. Otherwise it may enforce preexisting biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimReductionLDA(tfs_reduced, clusters, targetDim=2):\n",
    "    lda = LinearDiscriminantAnalysis(n_components=targetDim)\n",
    "    print('LDA:')\n",
    "    tfs_2d = %time lda.fit(tfs_reduced, clusters.labels_).transform(tfs_reduced)\n",
    "    return tfs_2d, lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "source": [
    "## tSNE\n",
    "*Summary*:\n",
    "\n",
    "\n",
    "*In-depth explanation*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimReductiontSNE(tfs_reduced, perplexity=30, learning_rate=100, targetDim=2):\n",
    "    print('t-SNE:')\n",
    "    tfs_2d = %time TSNE(n_components=targetDim, perplexity=perplexity, learning_rate=learning_rate).fit_transform(tfs_reduced)\n",
    "    return tfs_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linearize results into a grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapToSpaceSampling(points):\n",
    "    # just take the first n¬≤ < #points Points\n",
    "    points = points[: int(np.sqrt(len(points)))**2]\n",
    "    grid = np.dstack(np.meshgrid(np.linspace(np.min(points[:, 0]), np.max(points[:, 0]), int(np.sqrt(len(points)))),\n",
    "                       np.linspace(np.min(points[:, 1]), np.max(points[:, 1]), int(np.sqrt(len(points)))))).reshape(-1, 2)\n",
    "    cost = cdist(points, grid, \"sqeuclidean\").astype(np.float64)\n",
    "    print(cost.shape)\n",
    "    cost *= 100000 / cost.max()\n",
    "    row_ind_lapjv, col_ind_lapjv, _ = lapjv(cost, verbose=True, force_doubles=True)\n",
    "    return grid[row_ind_lapjv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeClusterTopography(points, values, width, height, interpolation='linear'):\n",
    "    # lay grid over the points so that all points are covered\n",
    "    grid_x, grid_y = np.mgrid[np.min(points[:,0]):np.max(points[:,0]):width*1j, np.min(points[:,1]):np.max(points[:,1]):height*1j]\n",
    "    return griddata(np.array(points), np.array(values[:len(points)]), (grid_x, grid_y), method=interpolation, fill_value=np.min(values[:len(points)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(tfs, emb_model, targetDim, dimreduction, clustering, embedding, num_topics, num_clusters, perplexity, learning_rate, error, interpolation, viz, width, height):\n",
    "    \n",
    "    if dimreduction == 'LSA':\n",
    "        tfs_reduced, red_model = LSA(tfs, num_topics=num_topics)\n",
    "    elif dimreduction == 'NMF':\n",
    "        tfs_reduced, red_model = NMF(tfs, num_topics=num_topics)\n",
    "    elif dimreduction == 'Autoencoder':\n",
    "        tfs_reduced, red_model = encoder.predict(tfs), None\n",
    "    else:\n",
    "        return 'No dimensionality reduction technique was selected!'\n",
    "    \n",
    "    if clustering == 'KMEANS':\n",
    "        clusters = cluster(tfs_reduced, num_topics=num_clusters)\n",
    "    else:\n",
    "        return 'No clustering technique was selected!'\n",
    "    \n",
    "    if embedding == 'LDA':\n",
    "        tfs_embedded, lda = dimReductionLDA(tfs_reduced, clusters=clusters, targetDim=targetDim)\n",
    "    elif embedding == 'tSNE':\n",
    "        tfs_embedded = dimReductiontSNE(tfs_reduced, perplexity=perplexity, learning_rate=learning_rate, targetDim=targetDim)\n",
    "    else:\n",
    "        return 'No dimensionality reduction technique was selected!'\n",
    "    \n",
    "    # compute linearization\n",
    "    tfs_mapped = mapToSpaceSampling(tfs_embedded) if targetDim == 2 else np.array([[0,0]]*len(tfs_embedded)) \n",
    "    \n",
    "    # compute top words\n",
    "    cluster_words = [emb_model.top_words(np.mean(tfs[clusters.labels_==cluster], axis=0), dct=dct, topn=5) for cluster in range(num_clusters)]\n",
    "    top_words = [emb_model.top_words(project, dct=dct, topn=5) for project in tfs]\n",
    "    # compute coherence score\n",
    "    cm = CoherenceModel(topics=cluster_words, corpus=mfncorpus, dictionary=dct, coherence='u_mass')\n",
    "    print(\"Coherence score: \", cm.get_coherence())\n",
    "    \n",
    "    #compute cluster topography\n",
    "    similarity_to_cluster_centers = [norm(x-clusters.cluster_centers_[clusters.labels_[i]]) for i,x in enumerate(tfs_reduced)]\n",
    "    similarity_to_cluster_centers = similarity_to_cluster_centers / -norm(similarity_to_cluster_centers)\n",
    "    reduction_error = np.max(lda.decision_function(tfs_reduced), axis=1) if (embedding == 'LDA') else [0]* len(tfs_embedded)\n",
    "    reduction_error = reduction_error / norm(reduction_error)\n",
    "    interpolated_topography = computeClusterTopography(tfs_embedded if viz == 'scatter' else tfs_mapped, similarity_to_cluster_centers if error=='cluster_error' else silhouette_samples(tfs_reduced, clusters.labels_), width, height, interpolation)\n",
    "    return tfs_reduced, clusters, tfs_embedded, tfs_mapped, cluster_words, top_words, similarity_to_cluster_centers, reduction_error, interpolated_topography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_scatter(data, width=600, height=600, viz='scatter'):\n",
    "    display(Javascript(\"\"\"\n",
    "        (function(element){\n",
    "            require(['scatter'], function(scatter) {\n",
    "                scatter(element.get(0), %s, %d, %d, %s);\n",
    "            });\n",
    "        })(element);\n",
    "    \"\"\" % (json.dumps(data), width, height, json.dumps(viz))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "outputs": [],
   "source": [
    "def save(payload):\n",
    "    name = \"c\" + str(payload['params']['num_clusters']) +\"-t\" + str(payload['params']['num_topics']) + \"_\" + str(payload['params']['embedding'])\n",
    "    if payload['params']['embedding'] == 'tSNE':\n",
    "        name += \"_p\" + str(payload['params']['perplexity']) + \"-lr\" + str(payload['params']['learning_rate'])\n",
    "    with open('./dumps/' + name + '.json', 'w') as dumpfile:\n",
    "        json.dump(payload, dumpfile, sort_keys=True, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "tags": [
      "remove_cell"
     ]
    }
   },
   "outputs": [],
   "source": [
    "def visualize(targetDim=2,tfs=None,dimreduction='LSA', clustering='KMEANS', embedding='LDA', num_topics=20, num_clusters=3, perplexity=5, learning_rate=200, error='cluster_error', interpolation='linear', viz='scatter', fake=''):\n",
    "    # viz dimensions\n",
    "    width = 600\n",
    "    height = 600\n",
    "    payload = {}\n",
    "    \n",
    "    \n",
    "    if not fake:\n",
    "        # compute all necessary stuff\n",
    "        tfs, model = pickle.loads(tfs)\n",
    "        tfs_reduced, clusters, tfs_embedded, tfs_mapped, cluster_words, top_words, similarity_to_cluster_centers, reduction_error, interpolated_topography = compute(tfs, model, targetDim, dimreduction, clustering, embedding, num_topics, num_clusters, perplexity, learning_rate, error, interpolation, viz, width, height)\n",
    "\n",
    "        [print(i, words) for i,words in enumerate(cluster_words)]\n",
    "        colours = d3['Category10'][num_clusters]\n",
    "        #ids, titles, texts = [list(elem) for elem in zip(*loadProjects())]\n",
    "        if targetDim == 2:\n",
    "            # configure bokeh plot                   \n",
    "            source = ColumnDataSource(data=dict(\n",
    "                x=tfs_embedded[:, 0],\n",
    "                y=tfs_embedded[:, 1],\n",
    "                x_mapped=tfs_mapped[:, 0],\n",
    "                y_mapped=tfs_mapped[:, 1],\n",
    "                ids=mfndata.getIDs(),\n",
    "                titles=mfndata.getTitles(),\n",
    "                colours=np.array(colours)[clusters.labels_],\n",
    "                labels=clusters.labels_\n",
    "            ))\n",
    "\n",
    "            TOOLTIPS = [\n",
    "                (\"index\", \"$index\"),\n",
    "                (\"id\", \"@ids\"),\n",
    "                (\"title\", \"@titles\"),\n",
    "            ]\n",
    "            # scatterplot\n",
    "            scatter = figure(plot_width=800, plot_height=800, title=None, toolbar_location=\"below\", tooltips=TOOLTIPS, tools='tap,pan,wheel_zoom,save')\n",
    "            scatter.scatter('x', 'y', size=10,color='colours', legend='labels', source=source)\n",
    "            url = 'http://gepris.dfg.de/gepris/projekt/@ids'\n",
    "            taptool = scatter.select(type=TapTool)\n",
    "            taptool.callback = OpenURL(url=url)\n",
    "\n",
    "            # mapped scatterplot\n",
    "            mapped_scatter = figure(plot_width=800, plot_height=800, title=None, toolbar_location=\"below\", tooltips=TOOLTIPS, tools='tap,pan,wheel_zoom')\n",
    "            mapped_scatter.scatter('x_mapped', 'y_mapped', size=50,color='colours', legend='labels', source=source)\n",
    "            url = 'http://gepris.dfg.de/gepris/projekt/@ids'\n",
    "            taptool = mapped_scatter.select(type=TapTool)\n",
    "            taptool.callback = OpenURL(url=url)\n",
    "            #show(row(scatter, mapped_scatter))\n",
    "        else:\n",
    "\n",
    "            source = go.Scatter3d(\n",
    "                x=tfs_embedded[:, 0],\n",
    "                y=tfs_embedded[:, 1],\n",
    "                z=tfs_embedded[:, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=2,\n",
    "                    color=clusters.labels_,                # set color to an array/list of desired values\n",
    "                    colorscale='Viridis',   # choose a colorscale\n",
    "                    opacity=0.8\n",
    "                )\n",
    "            )\n",
    "\n",
    "            data = [source]\n",
    "            layout = go.Layout(\n",
    "                margin=dict(\n",
    "                    l=0,\n",
    "                    r=0,\n",
    "                    b=0,\n",
    "                    t=0\n",
    "                )\n",
    "            )\n",
    "            fig = go.Figure(data=data, layout=layout)\n",
    "            iplot(fig, filename='3d-scatter-colorscale')\n",
    "\n",
    "        payload = {\n",
    "            'params': {\n",
    "                'targetDim': targetDim,\n",
    "                'dimreduction': dimreduction,\n",
    "                'clustering': clustering,\n",
    "                'embedding': embedding,\n",
    "                'num_topics': num_topics,\n",
    "                'num_clusters': num_clusters,\n",
    "                'perplexity': perplexity,\n",
    "                'learning_rate': learning_rate\n",
    "            },\n",
    "            'project_data': [{'id':pid,'reducedpoint': reducedpoint, 'embpoint':embpoint, 'mappoint':mappoint, 'cluster':cluster, 'error':error, 'title': title, 'words': words} for pid, reducedpoint, embpoint, mappoint, cluster, error, title, words in zip(\n",
    "                mfndata.getIDs(),\n",
    "                tfs_reduced.tolist(),\n",
    "                tfs_embedded.tolist(),\n",
    "                tfs_mapped.tolist(),\n",
    "                clusters.labels_.tolist(),\n",
    "                similarity_to_cluster_centers.tolist(),\n",
    "                mfndata.getTitles(),\n",
    "                top_words\n",
    "\n",
    "            )],\n",
    "            'cluster_data': {\n",
    "                'cluster_words': cluster_words,\n",
    "                'cluster_colour': colours\n",
    "            },\n",
    "            'cluster_topography': np.flip(interpolated_topography.T, axis=0).flatten().tolist()\n",
    "        }\n",
    "        save(payload)\n",
    "    else:\n",
    "        with open(fake, 'r') as input_data:\n",
    "            payload = payload=json.load(input_data)\n",
    "    display(HTML(filename=\"scatter.css.html\"))\n",
    "    display(Javascript(\"require.config({paths: {d3: 'https://d3js.org/d3.v5.min'}});\"))\n",
    "    display(Javascript(filename=\"scatter.js\"))\n",
    "    draw_scatter(payload, width, height, viz)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "local_metadata": {
     "scrolled": false
    },
    "remote_metadata": {
     "scrolled": false,
     "tags": [
      "remove_cell"
     ]
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ed4d77b89544e5b1f0995ffad6f7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=2, continuous_update=False, description='targetDim', max=3, min=2), Drop‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def s(x,y):\n",
    "    return IntSlider(min=x,max=y, value=(y-x)//2, continuous_update=False)\n",
    "\n",
    "w = interactive(visualize,targetDim=s(2,3),tfs=Dropdown(options=[('Doc2Vec', pickle.dumps((docs_vectorized_doc2vec, doc2vec_model))), ('TfIdf', pickle.dumps((docs_vectorized_tfidf, tfidf_model)))], value=pickle.dumps((docs_vectorized_doc2vec, doc2vec_model))), dimreduction=['LSA', 'NMF', 'Autoencoder'], clustering=['KMEANS'], embedding=['LDA', 'tSNE'], num_topics=s(4,48), num_clusters=s(4,10), perplexity=s(5,50), learning_rate=s(100,1000),error=['silhouette', 'cluster_error'], interpolation=['linear', 'cubic', 'nearest'], viz=['scatter', 'linearized'], fake='')\n",
    "output = w.children[-1]\n",
    "#output.layout.height = '2000px'\n",
    "display(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "ikon",
   "language": "python",
   "name": "ikon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
